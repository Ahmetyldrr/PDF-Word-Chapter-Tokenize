As mentioned earlier, the Transformer architecture makes use of layer normalization and skip connections. The former normalizes each input in the batch to have zero mean and unity variance. Skip connections pass a tensor to the next layer of the model without processing and add it to the processed tensor. When it comes to plac‐ing the layer normalization in the encoder or decoder layers of a transformer, there are two main choices adopted in the literature:
Post layer normalization 
This is the arrangement used in the Transformer paper; it places layer normaliza‐tion in between the skip connections. This arrangement is tricky to train from scratch as the gradients can diverge. For this reason, you will often see a concept known as learning rate warm-up, where the learning rate is gradually increased from a small value to some maximum value during training.
Pre layer normalization 
This is the most common arrangement found in the literature; it places layer nor‐malization within the span of the skip connections. This tends to be much more stable during training, and it does not usually require any learning rate warm-up.
The difference between the two arrangements is illustrated in Figure 3-6.
Figure 3-6. Different arrangements of layer normalization in a transformer encoder layer
We’ll use the second arrangement, so we can simply stick together our building blocks as follows:
classTransformerEncoderLayer(nn.Module): 
	def __init__(self, config): 
		super().__init__() 
		self.layer_norm_1=nn.LayerNorm(config.hidden_size) 		self.layer_norm_2=nn.LayerNorm(config.hidden_size) 		self.attention=MultiHeadAttention(config) 
		self.feed_forward=FeedForward(config)
defforward(self, x): 
# Apply layer normalization and then copy input into query, key, value hidden_state=self.layer_norm_1(x) 
# Apply attention with a skip connection 
x=x+self.attention(hidden_state) 
# Apply feed-forward layer with a skip connection 
x=x+self.feed_forward(self.layer_norm_2(x)) 
returnx
Let’s now test this with our input embeddings:
encoder_layer=TransformerEncoderLayer(config) 
inputs_embeds.shape, encoder_layer(inputs_embeds).size()
(torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))
We’ve now implemented our very first transformer encoder layer from scratch! How‐ever, there is a caveat with the way we set up the encoder layers: they are totally
invariant to the position of the tokens. Since the multi-head attention layer is effec‐tively a fancy weighted sum, the information on token position is lost.4
Luckily, there is an easy trick to incorporate positional information using positional embeddings. Let’s take a look.
