We’ve seen throughout this book that the self-attention mechanism plays a central role in the architecture of transformers; after all, the original Transformer paper is called “Attention Is All You Need”! However, there is a key challenge associated with self-attention: since the weights are generated from pairwise comparisons of all the tokens in a sequence, this layer becomes a computational bottleneck when trying to process long documents or apply transformers to domains like speech processing or computer vision. In terms of time and memory complexity, the self-attention layer of the Transformer architecture naively scales like n2, where n is the length of the sequence.5
As a result, much of the recent research on transformers has focused on making self-attention more efficient. The research directions are broadly clustered in Figure 11-4.
Figure 11-4. A summarization of research directions to make attention more efficient (courtesy of Yi Tay et al.)6
A common pattern is to make attention more efficient by introducing sparsity into the attention mechanism or by applying kernels to the attention matrix. Let’s take a quick look at some of the most popular approaches to make self-attention more effi‐cient, starting with sparsity.
