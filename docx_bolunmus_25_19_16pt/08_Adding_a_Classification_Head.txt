Transformer models are usually divided into a task-independent body and a task-specific head. We’ll encounter this pattern again in Chapter 4 when we look at the design pattern of  Transformers. What we have built so far is the body, so if we wish to build a text classifier, we will need to attach a classification head to that body. We have a hidden state for each token, but we only need to make one prediction. There are several options to approach this. Traditionally, the first token in such models is used for the prediction and we can attach a dropout and a linear layer to make a clas‐sification prediction. The following class extends the existing encoder for sequence classification:
classTransformerForSequenceClassification(nn.Module):
def __init__(self, config):
super().__init__()
self.encoder=TransformerEncoder(config)
self.dropout=nn.Dropout(config.hidden_dropout_prob)
self.classifier=nn.Linear(config.hidden_size, config.num_labels)
defforward(self, x):
x=self.encoder(x)[:, 0, :] # select hidden state of [CLS] token
x=self.dropout(x)
x=self.classifier(x)
returnx
Before initializing the model we need to define how many classes we would like to predict:
config.num_labels=3
encoder_classifier=TransformerForSequenceClassification(config)
encoder_classifier(inputs.input_ids).size()
torch.Size([1, 3])
That is exactly what we have been looking for. For each example in the batch we get the unnormalized logits for each class in the output. This corresponds to the BERT model that we used in Chapter 2 to detect emotions in tweets.
This concludes our analysis of the encoder and how we can combine it with a task-specific head. Let’s now cast our attention (pun intended!) to the decoder.
