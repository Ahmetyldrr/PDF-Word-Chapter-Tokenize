Throughout this book we’ve explored the powerful capabilities of transformers across a wide range of NLP tasks. In this final chapter, we’ll shift our perspective and look at some of the current challenges with these models and the research trends that are try‐ing to overcome them. In the first part we explore the topic of scaling up transform‐ers, both in terms of model and corpus size. Then we turn our attention toward various techniques that have been proposed to make the self-attention mechanism more efficient. Finally, we explore the emerging and exciting field of multimodal transformers, which can model inputs across multiple domains like text, images, and audio.
