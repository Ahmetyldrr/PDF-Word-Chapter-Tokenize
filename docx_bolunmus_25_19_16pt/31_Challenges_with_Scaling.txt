While scaling up sounds simple in theory (“just add more layers!”), in practice there are many difficulties. Here are a few of the biggest challenges you’re likely to encounter when scaling language models:
Infrastructure 
Provisioning and managing infrastructure that potentially spans hundreds or thousands of nodes with as many GPUs is not for the faint-hearted. Are the required number of nodes available? Is communication between nodes a bottle‐neck? Tackling these issues requires a very different skill set than that found in most data science teams, and typically involves specialized engineers familiar with running large-scale, distributed experiments.
Cost 
Most ML practitioners have experienced the feeling of waking up in the middle of the night in a cold sweat, remembering they forgot to shut down that fancy GPU on the cloud. This feeling intensifies when running large-scale experiments, and most companies cannot afford the teams and resources necessary to train models at the largest scales. Training a single GPT-3-sized model can cost several million dollars, which is not the kind of pocket change that many companies have lying around.4
Dataset curation 
A model is only as good as the data it is trained on. Training large models requires large, high-quality datasets. When using terabytes of text data it becomes harder to make sure the dataset contains high-quality text, and even preprocessing becomes challenging. Furthermore, one needs to ensure that there is a way to control biases like sexism and racism that these language models can acquire when trained on large-scale webtext corpora. Another type of considera‐tion revolves around licensing issues with the training data and personal infor‐mation that can be embedded in large text datasets.
Model evaluation 
Once the model is trained, the challenges don’t stop. Evaluating the model on downstream tasks again requires time and resources. In addition, you’ll want to probe the model for biased and toxic generations, even if you are confident that you created a clean dataset. These steps take time and need to be carried out thoroughly to minimize the risks of adverse effects later on.
4 However, recently a distributed deep learning framework has been proposed that enables smaller groups to pool their computational resources and pretrain models in a collaborative fashion. See M. Diskin et al., , (2021).
Deployment 
Finally, serving large language models also poses a significant challenge. In Chap‐ter 8 we looked at a few approaches, such as distillation, pruning, and quantiza‐tion, to help with these issues. However, this may not be enough if you are starting with a model that is hundreds of gigabytes in size. Hosted services such as the  or Hugging Face’s  are designed to help cat cannot or do nese deployment challenges.
This is by no means an exhaustive list, but it should give you an idea of the kinds of considerations and challenges that go hand in hand with scaling language models to ever larger sizes. While most of these efforts are centralized around a few institutions that have the resources and know-how to push the boundaries, there are currently two community-led projects that aim to produce and probe large language models in the open:
BigScience 
This is a one-year-long research workshop that runs from 2021 to 2022 and is focused on large language models. The workshop aims to foster discussions and reflections around the research questions surrounding these models (capabilities, limitations, potential improvements, bias, ethics, environmental impact, role in the general AI/cognitive research landscape) as well as the challenges around cre‐ating and sharing such models and datasets for research purposes and among the research community. The collaborative tasks involve creating, sharing, and evalu‐ating a large multilingual dataset and a large language model. An unusually large compute budget was allocated for these collaborative tasks (several million GPU hours on several thousands GPUs). If successful, this workshop will run again in the future, focusing on involving an updated or different set of collaborative tasks. If you want to join the effort, you can find more information at the .
EleutherAI 
This is a decentralized collective of volunteer researchers, engineers, and devel‐opers focused on AI alignment, scaling, and open source AI research. One of its aims is to train and open-source a GPT-3-sized model, and the group has already released some impressive models like  and , which is a 6-billion-parameter model and currently the orminlicly available trans‐former in terms of zero-shot performance. You can find more information at EleutherAI’s .
Now that we’ve explored how to scale transformers across compute, model size, and dataset size, let’s examine another active area of research: making self-attention more efficient.
