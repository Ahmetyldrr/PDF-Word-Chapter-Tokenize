The feed-forward sublayer in the encoder and decoder is just a simple two-layer fully connected neural network, but with a twist: instead of processing the whole sequence of embeddings as a single vector, it processes each embedding independently. For this reason, this layer is often referred to as a position-wise feed-forward layer. You may also see it referred to as a one-dimensional convolution with a kernel size of one, typ‐ically by people with a computer vision background (e.g., the OpenAI GPT codebase uses this nomenclature). A rule of thumb from the literature is for the hidden size of the first layer to be four times the size of the embeddings, and a GELU activation function is most commonly used. This is where most of the capacity and memoriza‐tion is hypothesized to happen, and it’s the part that is most often scaled when scaling up the models. We can implement this as a simple nn.Module as follows:
classFeedForward(nn.Module):
def __init__(self, config):
super().__init__()
self.linear_1=nn.Linear(config.hidden_size, config.intermediate_size)
self.linear_2=nn.Linear(config.intermediate_size, config.hidden_size)
self.gelu=nn.GELU()
self.dropout=nn.Dropout(config.hidden_dropout_prob)
defforward(self, x):
x=self.linear_1(x)
x=self.gelu(x)
x=self.linear_2(x)
x=self.dropout(x)
returnx
Note that a feed-forward layer such as nn.Linear is usually applied to a tensor of shape (batch_size, input_dim), where it acts on each element of the batch dimen‐sion independently. This is actually true for any dimension except the last one, so when we pass a tensor of shape (batch_size, seq_len, hidden_dim) the layer is applied to all token embeddings of the batch and sequence independently, which is exactly what we want. Let’s test this by passing the attention outputs:
feed_forward=FeedForward(config)
ff_outputs=feed_forward(attn_outputs)
ff_outputs.size()
torch.Size([1, 5, 768])
We now have all the ingredients to create a fully fledged transformer encoder layer! The only decision left to make is where to place the skip connections and layer nor‐malization. Let’s take a look at how this affects the model architecture.
