A lot of data, such as customer data within a company, is stored in structured data‐bases instead of as raw text. We saw in Chapter 7 that with question answering mod‐els we can query text with a question in natural text. Wouldn’t it be nice if we could do the same with tables, as shown in Figure 11-10?
Figure 11-10. Question answering over a table (courtesy of Jonathan Herzig)
TAPAS (short for Table Parser)13 to the rescue! This model applies the Transformer architecture to tables by combining the tabular information with the query, as illus‐trated in Figure 11-11.
Figure 11-11. Architecture of TAPAS (courtesy of Jonathan Herzig)
Let’s look at an example of how TAPAS works in practice. We have created a fictitious version of this book’s table of contents. It contains the chapter number, the name of the chapter, as well as the starting and ending pages of the chapters:
book_data= [
 {"chapter": 0, "name": "Introduction", "start_page": 1, "end_page": 11}, {"chapter": 1, "name": "Text classification", "start_page": 12, 
	"end_page": 48},
 {"chapter": 2, "name": "Named Entity Recognition", "start_page": 49, 	"end_page": 73},
 {"chapter": 3, "name": "Question Answering", "start_page": 74,
		"end_page": 120},
 {"chapter": 4, "name": "Summarization", "start_page": 121, 		"end_page": 140},
 {"chapter": 5, "name": "Conclusion", "start_page": 141, 		"end_page": 144} 
]
We can also easily add the number of pages each chapter has with the existing fields. In order to play nicely with the TAPAS model, we need to make sure that all columns are of type str:
table=pd.DataFrame(book_data) 
table['number_of_pages'] =table['end_page']-table['start_page'] table=table.astype(str) 
table
By now you should know the drill. We first load the table-question-answering pipeline:
table_qa=pipeline("table-question-answering")
and then pass some queries to extract the answers:
table_qa=pipeline("table-question-answering") 
queries= ["What's the topic in chapter 4?", 
"What is the total number of pages?", 
"On which page does the chapter about question-answering start?", "How many chapters have more than 20 pages?"] 
preds=table_qa(table, queries)
These predictions store the type of table operation in an aggregator field, along with the answer. Let’s see how well TAPAS fared on our questions:
forquery, predinzip(queries, preds): 
	print(query) 
	ifpred["aggregator"] =="NONE": 
		print("Predicted answer: "+pred["answer"]) 	else: 
		print("Predicted answer: "+pred["answer"]) 	print('='*50)
What's the topic in chapter 4?
Predicted answer: Summarization
==================================================
What is the total number of pages?
Predicted answer: SUM > 10, 36, 24, 46, 19, 3
==================================================
On which page does the chapter about question-answering start?
Predicted answer: AVERAGE > 74
==================================================
How many chapters have more than 20 pages?
Predicted answer: COUNT > 1, 2, 3
==================================================
For the first chapter, the model predicted exactly one cell with no aggregation. If we look at the table, we see that the answer is in fact correct. In the next example the model predicted all the cells containing the number of pages in combination with the sum aggregator, which again is the correct way of calculating the total number of pages. The answer to question three is also correct; the average aggregation is not necessary in that case, but it doesn’t make a difference. Finally, we have a question that is a little bit more complex. To determine how many chapters have more than 20 pages we first need to find out which chapters satisfy that criterion and then count them. It seem that TAPAS again got it right and correctly determined that chapters 1, 2, and 3 have more than 20 pages, and added a count aggregator to the cells.
The kinds of questions we asked can also be solved with a few simple Pandas com‐mands; however, the ability to ask questions in natural language instead of Python code allows a much wider audience to query the data to answer specific questions. Imagine such tools in the hands of business analysts or managers who are able verify their own hypotheses about the data!
