In 2019, the researcher  wrote a provocative essay entitled  in which he arg
The biggest lesson that can be read from 70 years of AI research is that general meth‐ods that leverage computation are ultimately the most effective, and by a large mar‐gin… . Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation. These two need not run counter to each other, but in practice they tend to… . And the human-knowledge approach tends to complicate methods in ways that make them less suited to taking advantage of general methods leveraging computation.
The essay provides several historical examples, such as playing chess or Go, where the approach of encoding human knowledge within AI systems was ultimately outdone by increased computation. Sutton calls this the “bitter lesson” for the AI research field:
75
We have to learn the bitter lesson that building in how we think we think does not work in the long run…. One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning.
There are now signs that a similar lesson is at play with transformers; while many of the early BERT and GPT descendants focused on tweaking the architecture or pre‐training objectives, the best-performing models in mid-2021, like GPT-3, are essen‐tially basic scaled-up versions of the original models without many architectural modifications. In Figure 11-1 you can see a timeline of the development of the largest models since the release of the original Transformer architecture in 2017, which shows that model size has increased by over four orders of magnitude in just a few years!
Figure 11-1. Parameter counts over time for prominent Transformer architectures
This dramatic growth is motivated by empirical evidence that large language models perform better on downstream tasks and that interesting capabilities such as zero-shot and few-shot learning emerge in the 10- to 100-billion parameter range. How‐ever, the number of parameters is not the only factor that affects model performance; the amount of compute and training data must also be scaled in tandem to train these monsters. Given that large language models like GPT-3 are estimated to cost  to train, it is clearly desirable to be able to estimate the model’s performnce. Somewhat surprisingly, the performance of language models appears to
obey a power law relationship with model size and other factors that is codified in a set of scaling laws.1 Let’s take a look at this exciting area of research.
